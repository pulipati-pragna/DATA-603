# Assignment_1


## BIG DATA WITH EXAMPLE AND TYPES

**_Big Data_** : As the word says, it is the datasets which are bigger in quantities. Due to the data being too large, it is not possible to store, analyze and manipulate data in a short span of time. In the world full of internet users, big data is everywhere. Social media platforms such as Facebook, Whatsapp, Instagram, etc generate data in millions per second that has to be stored in order to analyse and improve for better usage. This data can be in various forms such as photos, videos, texts, locations and documents. Besides this we can also store financial transaction data from banking applications.

**_Types_** : Types of big data include batches, streamings, graphs, spatio temporal data and many more.


## 6 V'S OF BIG DATA (DEFINE EACH)

**_Volume_** : Volume refers to the amount of data that is retrieve from various sources. Multiple machines are used to store the data.

**_Velocity_** : Velocity refers to the speed at which the data is generated at the sources. In social media applications and sensor applications, the amount of data recorded is very high resulting the data ot have a high velocity.

**_Variety_** : Variety refers to the various forms that the data can be obtained, stored or analysed in. The data can be structured, unstructured or semi structured. And the data elements can be in the form of pictures, text, data, metadata, audio, video, etc.

**_Variability_** : Variability can be defined as the ways in which the data can be made useful or the ways in which it can be formatted.

**_Veracity_** : Veracity measures the quality and accuracy of the data. It can be defined as degree of data that can be trusted. To have high veracity, the data needs to be clear from anamolies, noise and unwanted data.

**_Value_** : Value refers the amount of importance the data brings into the prediction model.


## PHASES OF BIG DATA ANALYSIS (DISUSS EACH)

**_Phase 1 / Data Acquisition and Recovery_** : In this part of analysis, we obtain the real time data into the sources. From data generators such as IoT sensors, social media platforms, sattelite and etc, we can gather the data. Metadata is also one such data generator which consists of the details of the data such as location from which is data is being sent, the time at which it is sent and etc.

**_Phase 2 / Information Extraction and Cleaning / Annotation_** : In this phase, we extract the required information from the data sources and demonstrate it in a structural and meaningful form. To obtain this, we need to clean the data and fill in the missing values. Identifying the outliers helps correcting the inconsistencies in the data.

**_Phase 3 / Data Integration, Aggregation and Representation_*** : Here, we merge the data from different sources that have been extracted and cleaned in the previous phase. The data from all the sources cannot be expected to be homogeneous. These data can be represented in the form of box plot, bar graph, histogram or piechart for a better understanding.

**_Phase 4 / Query Processing, Data Modeling and Analysis_** : We use query languages to query the data to retrieve useful subsets inorder ot understand various trends. An overall abstract of the data is organized into a model standardizing the relation between the data themselves. Data models can be represented as database model, entity relationship model, semantic data model and etc. In order to query the data, there are languages other than SQL.

**_Phase 5 / Interpretation_** : From the obtained results after building a model and testing it, will be understood and verified which helps in the process of decision making and identifying trends. Even using visualization.

## CHALLENGES IN BIG DATA ANALYSIS (DISCUSS EACH)

**_Challenge 1 / Heterogenity and Incompleteness_** : The machines expect the all of the gathered data to be homogeneous, and tend to cause ambiguity when there are differences or heterogenity in them. For semi-structured data, processing needs to be done in order to analyse them. Even after cleaning and correcting the data, errors and null values might remain and they must be taken care of in the further data analysis steps.

**_Challenge 2 / Scale_** : The amount of data being generated and stored is way bigger than the amount of data that is computed and analysed.

**_Challenge 3 / Timeliness_** : When a model is run, we expect the results to be immediate and accurate. But for larger datasets, time is taken to fetch data, apply the rules and analyse the output and extract trends. The fetch time and analysis time would get easier if the data is homogeneous and structured.

**_Challenge 4 / Privacy_** : On one side, we must protect the data but on the other hand, Using data sharing, data analysis is done without any regard to privacy of information. When the dataset is small, the issue of privacy was also not much to be worried about. But as the data increases, the issue of privacy concern also increases exponentially.

**_Challenge 5 / Human Collaboration_** : In big data analysis, both computer and human analysis is involved. The model must be able to take reviews and inputs from the experts and explore the data for more favourable and precise outcomes. When the human analysis inputs are being gathered from various crowd sources, there is a high chance of error, uncertainity and confliction.


__SOURCE - Introduction to Big Data Analytics, DATA603 Big Data Platforms, UMBC by Dr. Najam Hassan__
